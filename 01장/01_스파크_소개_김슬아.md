# [01] 스파크 소개

# 1.1 스파크

## ~~1.1.1 빅데이터의 등장, 1.1.2 빅데이터의 정의~~

## 1.1.3 빅데이터 솔루션, 1.1.4 스파크

---

- **데이터 수집**: Flume, Kafka, SQOOP
- **데이터 저장 및 처리**: Hadoop, HBase, Cassandra, Redis, Pig, Hive, Spark
    - Hadoop: HDFS(분산 파일 저장 시스템), MapReduce(분산 데이터 처리 프로그래밍 모델이자 프레임워크)를 제공하는 분산 환경의 병렬처리 프레임워크
        - 하나의 네임노드와 여러개의 데이터노드로 구성
        - 데이터를 저장할 때 전체 데이터를 `블록`이라고 하는 일정한 크기로 나누어 데이터 노드에 분산하여 저장
            - 각 `블록`들이 어느 데이터 노드에 저장되어 있는지는 네임노드에 메타정보로 기록
    - Spark: 클러스터 기반의 분산 처리 기능을 제공하는 프레임워크(Hadoop과 유사)
        - Map과 Reducer 패턴으로는 다양한 데이터 분석 요구사항을 처리하기 어려웠던 Hadoop 기반 MapReduce 작업의 단점을 보완
        - 2012년 미국 NSDI 학회에서 Spark 핵심 개념인 RDD(resilient distributed dataset)에 대한 논문이 발표되면서 알려짐
    - Hadoop과 Spark의 차이점: 처리 결과를 Hadoop은 파일시스템에 유지하나, Spark는 메모리에 저장하고 관리
- **데이터 분석 및 기타 소프트웨어**: R, Cloudera, Hortonworks, Elasticsearch

## 1.1.5 RDD 소개와 연산

---

- **RDD(resilient distributed dataset)란?**
    - 분산 저장된 데이터들의 집합
    - 병렬처리가 가능하며, 장애(프로그램 오류 제외)가 발생하면 스스로 복구가 가능
    - 값에 해당하는 데이터 뿐만 아니라 데이터 처리 방법 및 실제적인 데이터 처리 능력도 가진 모델
    - RDD를 생성하는 작업을 기록해 둠(lineage)
    - RDD에 속한 요소들은 `파티션`이라는 단위로 나눠질 수 있음(Spark가 작업을 수행할 때 파티션 단위로 병렬 처리)
    - `파티션`으로 나뉘어져 다수의 서버에서 실행되다 보니 `파티션` 처리 장애로 처리 결과가 유실되면 기록해둔 작업 내용을 바탕으로 문제가 생겼던 RDD 생성 작업을 다시 수행
- **RDD 생성 방식**
    1. 기존 프로그램의 메모리에 생성된 데이터 이용
    2. 로컬 파일시스템, Hadoop의 HDFS와 같은 외부 저장소에 저장된 데이터
    3. 기존에 생성되어있던 RDD로부터 새로운 RDD를 생성(연산으로 인한 생성)
- **RDD 연산 종류**
    - Transformation 연산: 기존 RDD는 바뀌지 않은 채 변형된 값을 가진 새로운 RDD가 생성되는 연산
        - lazy 실행 방식: 데이터 변환 수행 방식에 대한 정보만 누적해서 가지고 있다가, Action에 해당하는 연산이 호출될 때 Transformation의 연산이 수행됨
            - 장점: 절차에 따른 데이터 변형을 알 수 있음, 변환 연산의 최적화 가능(ex. 데이터가 네트워크를 통해 이동하는 현상인 shuffle 최소화)
    - Action 연산: 연산의 결과가 RDD가 아닌 다른 값이거나, 반환값이 없는 연산
    - Transformation과 Action 연산 구분: 메서드의 반환 타입을 확인
        - 수행 결과가 RDD이면 Transformation, RDD 외의 다른 타입의 값이면 Action

## 1.1.6 DAG

---

- **DAG(Directed Acyclic Graph)란?**
    - 여러 개의 꼭짓점 또는 노드와 그 사이를 이어주는 방향성을 지닌 선으로 구성됨
    - 그래프의 어느 꼭짓점이나 노드에서 출발하더라도 다시 원래의 꼭짓점으로 돌아오지 않는 그래프 모델
- 빅데이터에서 복잡한 일련의 작업 흐름을 나타내기 위해 DAG를 주로 사용

    ![](jech-2008-September-62-9-842-F2-ea453f19-674e-48ee-9e92-e3ce97061574.large.jpg)

    [https://jech.bmj.com/content/62/9/842](https://jech.bmj.com/content/62/9/842)

- 스파크 DAG 스케줄러, Spark의 작업 실행 방식
    - 전체 작업을 `Stage`라는 단위로 나눈 뒤 다시 `Stage`를 `task`로 나누어서 작업을 실행
    - 메인 함수를 실행해 RDD 등을 생성하고 각종 연산을 호출하는 드라이버 프로그램(main 함수를 가진 일반적인 프로그램)을 작성
    - narrow dependency, wide dependency

## 1.1.7 람다 아키텍처

---

- 람다 아키텍처란? 데이터를 처리하는 시스템을 일괄 처리하는 Batch Layer와 실시간으로 처리하는 Speed Layer로 나눔
    - 대량의 데이터 처리와 실시간 로그 분석과 같은 실시간 처리 두 가지 요구사항을 모두 충족시키기 위함
    - 실시간과 배치 뷰를 적절히 조합하고 장애 복구 상황에 대처하는 방법이 람다 아키텍처의 포인트

# ~~1.2 스파크 설치, 1.3 개발 환경 구축, 1.4 예제 프로젝트 실행~~

# 1.5 데이터프레임과 데이터셋

    val wordDF = df.select(explode(split(col("value"), " ")).as("word") // RDD
    val result = wordDF.groupBy("word").count // Dataframe

- RDD - 메서드 인자에 데이터 처리를 위한 함수를 직접 작성해서 전달
- Dataframe - 내장함수와 표현식 등을 통한 간접적인 처리 방식을 사용
    - RDD보다 더욱 높은 수준의 최적화 가능
- Dataset - Dataframe과 큰 차이가 없음(코드 작성하는 방식만 다름)