# 스파크 2 프로그래밍 정리

- ## 3장 클러스터 환경.
    1. 클러스터 모드와 컴포넌트.
        - 용어 정리
            1. 로컬 모드 = 한대의 서버에서 동작할때
            2. 클러스터 모드 = 다수의 서버에서 동작할때
            3. 클러스터 매니저 = 클러스터 환경에서 전체 서버의 
            자원과 동작을 세밀하고 효율적으로 제어하는 역할의 별도 모듈
            4. 마스터 서버 = 스파크 컨텍스트 생성 및 워커노드에게 역할 수행 전달.
            5. 워커 노드 = 마스터 서버에게 작업을 전달받아 수행하는 서버, 워커노드는 익스큐터라는 프로세스를 생성하여 작업을 수행한다.
            6. Job = 트랜스포메이션작업~액션연산을 의미한다.
            7. 스테이지 = Job을 셔플의 필요 여부(=Wide Dependency)에 따라 나누는 단위
            8. 태스크 = 한개의 스테이지는 여러 태스크로 구성. 이 태스크가 실제로 익스큐터에 할당되는 작업 단위.
            9. 익스큐터 = 할당 받은 작업을 처리하는 것과, 재사용을 위해 처리한 작업을 메모리에 저장하는 2가지로 나뉩니다.
            
        - 스파크는 자체적으로 제공하는 클러스터 매니저와 외부 클러스터 매니저 라이브러리 모두 사용 가능.
        - 스파크는 각 다른 클러스터 매니저를 일관된 방법으로 사용하기 위해 추상화된 클러스터 매니저를 제공.
        - 스파크 프로그램은 논리적으로 마스터, 워커노드로 분리된다.
        - 논리적인 관점에서 마스터를 클러스터 매니저로 부르며, sparkContext를 생성하여 각 워커노드에 익스큐터(=process)들을 생성하여 수행하도록 한다.
        
        - 스파크 컨텍스트(=대리자)의 역할
            1. 스파크 작업의 시작점.
            2. 스파크 클러스터와의 연결
            3. RDD, 브로드캐스트, 어큐뮬레이터의 사용 제공.
        - 스파크 작업은 하나의 스파크 컨텍스트만을 사용해야한다.
            - 각 스파크 작업은 간섭없이 수행이 가능하다는 장점.
            - 각 스파크 작업간의 공통된 데이터를 공유하지 못한다는 단점.
    2. 클러스터 모드를 위한 시스템 구성
        - 배치/클라이언트 서버란? = 스파크 클러스터에 작업을 요청하는 서버
        - 스파크의 경우 일반적으로 클러스터서버, 클라이언트 서버가 존재하여 클라이언트 서버가 클러스터 서버에 요청을 하는 방식을 사용합니다.
        - 로컬 개발 서버란? = 말 그대로 로컬에서 프로그램을 실행하는 서버를 말합니다. 특별한 제약이 없다면 로컬 개발 서버에서 스파크 클러스터 서버에 요청할 수 있습니다.<br>
        하지만, 일반 회사들은 대게 로컬과 클러스터 서버는 네트워크제약이 있어, 로컬 개발 서버를 드라이버 프로그램을 수행하는 서버로 사용하기에는 부적합합니다.

        - 애플리케이션 실행 서버란? = 이 책에서 정의한 이름으로 스파크 애플리케이션을 처음 실행하는 서버를 일컫습니다.
        <br>
        스파크 애플리케이션은 애플리케이션 실행 서버에서 모두 이루어 지는 것이 아니고, 드라이버 프로그램과 워커노드간에 이루어 지는것입니다.

        - 클러스터 서버란? = 스파크 애플리케이션을 수행하기 위해 clustering된 서버들을 일컫습니다. <br>
        (마스터 + 워커)노드라고 생각하면 됩니다.

    3. 드라이버 프로그램과 디플로이 모드
        - 드라이버 프로그램이란? = 스파크 컨텍스트를 생성하는 코드가 포함된 프로그램을 일컫습니다.
        - 드라이버 프로그램은 클러스터 매니저에게 작업을 요청하고,
        클러스터 매니저는 받은 작업을 디플로이 모드에 맞게 워커노드들에게 작업을 할당합니다.
        - 디플로이 모드의 종류
            1. 클라이언트 디플로이모드 = 애플리케이션을 실행한 프로세스내부에서 드라이버 프로그램을 구동하는 것을 일컫습니다.
            2. 클러스터 디플로이모드 = 애플리케이션을 실핸한 프로세스가 클러스터 매니저에게 작업만 요청하고 종료하고, 실제 작업은 클러스터 내부에서 일어나는 것을 일컫습니다.
        - 클라이언트의 경우 애플리케이션을 실행한 서버에서 sc(스파크 컨텍스트)가 생성되므로 프로세스를 kill하게 되면 모든 스파크 잡이 중지되게 됩니다.
        - 클러스터의 경우에는 작업의 요청만을 보내고 실제 sc(스파크 컨텍스트)는 클러스터 서버중 하나에서 생성되므로 애플리케이션을 실행하는 프로세스를 kill하더라도 영향이 가지 않습니다.

        - 일반적으로 정기적인 작업은 클러스터 디플로이 모드를 사용하며, 인터렉티브하게 스파크 작업의 내용을 확인하는 경우에는 클라이언트 디플로이모드를 사용합니다.

        - 클라이언트 디플로이모드는 클러스터 서버군과 실제로 네트워크 거리가 있으면 영향이 갈 수 있어, 되도록 가까운 곳에서 애플리케이션을 수행하도록 해야합니다.

- ## cloudera manager를 통한 멀티 클러스터 구축.
    1. cm을 사용하여 구축할때는 사전작업이 몇가지 필요하다(sudo command가 가능한 계정으로 진행해야한다.)
        
        - ssh 연결
            1. ssh-keygen command을 통해 키 생성
            2. 마스터 서버(cm을 설치하는 서버를 분산 클러스터의 마스터 서버로 사용하였음)의 public key를 각 슬레이브 서버의 authorized_keys에 추가
                - authorized_keys는 ~/.ssh 디렉토리에 위치하도록 함.
            3. 최초 ssh 연결시 yes or no를 물어보기 때문에 최초 접속 후 cm을 이용하여 설치 진행.

        - symbolic link(/ disk full을 방지하기 위해) -> destination dir은 777로 chmod하였다.

            0. 저의 경우 destination을 위해  ```~/../cdh```  dir를 만들었습니다.
            1. log 관련 link 걸기
                - /opt/cloudera
                - /dfs
                - /impala
                - /yarn
                - /var/log/ [spark, statestore, zookeeper, hive, catalogd, cloudera-manager-installer, cloudera-scm-agent, cloudera-scm-alertpublisher, cloudera-scm-eventserver, cloudera-scm-firehose, hadoop-hdfs, hadoop-mapreduce, hadoop-yarn ... 자신이 사용할 서비스]
                - /var/lib [cloudera-host-monitor, cloudera-scm-eventserver, cloudera-service-monitor, hadoop-yarn, zookeeper ... 자신이 사용할 서비스]
                - ``` 서비스가 추가된다면? mv -> rmdir -> add symbolic link```
                -
                ```shell
                sudo mkdir -p /home1/cdh/opt/cloudera
                sudo mkdir -p /home1/cdh/log/catalogd
                sudo mkdir -p /home1/cdh/log/cloudera-scm-eventserver
                sudo mkdir -p /home1/cdh/log/hadoop-mapreduce
                sudo mkdir -p /home1/cdh/log/hue
                sudo mkdir -p /home1/cdh/log/statestore
                sudo mkdir -p /home1/cdh/log/cloudera-manager-installer
                sudo mkdir -p /home1/cdh/log/cloudera-scm-firehose
                sudo mkdir -p /home1/cdh/log/hadoop-yarn
                sudo mkdir -p /home1/cdh/log/impalad
                sudo mkdir -p /home1/cdh/log/zookeeper
                sudo mkdir -p /home1/cdh/log/cloudera-scm-agent 
                sudo mkdir -p /home1/cdh/log/hadoop-hbase
                sudo mkdir -p /home1/cdh/log/hbase
                sudo mkdir -p /home1/cdh/log/oozie
                sudo mkdir -p /home1/cdh/log/cloudera-scm-alertpublisher
                sudo mkdir -p /home1/cdh/log/hadoop-hdfs
                sudo mkdir -p /home1/cdh/log/hive
                sudo mkdir -p /home1/cdh/log/spark
                sudo mkdir -p /home1/cdh/dfs/nn 
                sudo mkdir -p /home1/cdh/yarn/nm
                sudo mkdir -p /home1/cdh/cloudera-host-monitor 
                sudo mkdir -p /home1/cdh/cloudera-service-monitor 
                sudo mkdir -p /home1/cdh/lib/hadoop-yarn/yarn/nm-recovery 
                sudo mkdir -p /home1/cdh/lib/zookeeper
                sudo mkdir -p /home1/cdh/lib/oozie
                sudo mkdir -p /home1/cdh/lib/cloudera-scm-eventserver
                sudo mkdir -p /home1/cdh/tmp 
                sudo mkdir -p /home1/cdh/impala
                sudo mkdir -p /home1/cdh/impala/impalad
                sudo mkdir -p /home1/cdh/solr

                sudo ln -s /home1/cdh/opt/cloudera/ /opt/cloudera
                sudo ln -s /home1/cdh/log/cloudera-scm-agent /var/log/cloudera-scm-agent 
                sudo ln -s /home1/cdh/log/catalogd /var/log/catalogd
                sudo ln -s /home1/cdh/log/cloudera-manager-installer /var/log/cloudera-manager-installer
                sudo ln -s /home1/cdh/log/cloudera-scm-agent /var/log/cloudera-scm-agent
                sudo ln -s /home1/cdh/log/cloudera-scm-alertpublisher /var/log/cloudera-scm-alertpublisher
                sudo ln -s /home1/cdh/log/cloudera-scm-eventserver /var/log/cloudera-scm-eventserver
                sudo ln -s /home1/cdh/log/cloudera-scm-firehose /var/log/cloudera-scm-firehose
                sudo ln -s /home1/cdh/log/hadoop-hbase /var/log/hadoop-hbase
                sudo ln -s /home1/cdh/log/hadoop-hdfs /var/log/hadoop-hdfs
                sudo ln -s /home1/cdh/log/hadoop-mapreduce /var/log/hadoop-mapreduce
                sudo ln -s /home1/cdh/log/hadoop-yarn /var/log/hadoop-yarn
                sudo ln -s /home1/cdh/log/hbase /var/log/hbase
                sudo ln -s /home1/cdh/log/hive /var/log/hive
                sudo ln -s /home1/cdh/log/impalad /var/log/impalad
                sudo ln -s /home1/cdh/log/oozie /var/log/oozie
                sudo ln -s /home1/cdh/log/spark /var/log/spark
                sudo ln -s /home1/cdh/log/statestore /var/log/statestore
                sudo ln -s /home1/cdh/log/zookeeper /var/log/zookeeper
                sudo ln -s /home1/cdh/lib/hadoop-yarn/ /var/lib/hadoop-yarn
                sudo ln -s /home1/cdh/lib/zookeeper /var/lib/zookeeper
                sudo ln -s /home1/cdh/cloudera-host-monitor  /var/lib/cloudera-host-monitor
                sudo ln -s /home1/cdh/cloudera-service-monitor /var/lib/cloudera-service-monitor
                sudo ln -s /home1/cdh/dfs/ /dfs
                sudo ln -s /home1/cdh/yarn/ /yarn
                sudo ln -s /home1/cdh/impala/ /impala
                sudo ln -s /home1/cdh/tmp /tmp
                
                sudo chmod 777 -R /home1/cdh

                ```

        - /etc/hosts 수정
            1. 마스터 서버의 /etc/hosts 파일에 슬레이브 서버의 ip, domain을 추가.<br>
            ```
            ##.###.###.###  dev-geon-cloudera002-###.###.##
            ##.###.###.###  dev-geon-cloudera003-###.###.##
            ##.###.###.###  dev-geon-cloudera004-###.###.##

            ```
    2. cloudera manager installer binary 다운로드 및 실행.
        - ```wget https://archive.cloudera.com/cm6/6.2.0/cloudera-manager-installer.bin```
        - ```chmod 755 cloudera-manager-installer.bin```
        - ```sudo ./cloudera-manager-installer.bin```
        - install 화면은 모두 yes를 하여 설치 진행(oracle jdk, database 등등을 설치하며 cm process를 띄운다) : 7180 port로 웹 UI에 접근 가능.

    3. 7180 포트로 UI에 들어가 설치 진행.
        - 호스트 textbox = 호스트(마스터 & 슬레이브 서버)의 ip 혹은 domain리스트를 line by line으로 넣음
        - ssh로 사용하여 설치.
            - username은 sudo가 가능한 계정명 기입
            - key file을 선택(마스터 서버의 private key 파일)
            - 설치 진행.
    4. 설치 후 각 서비스의 heapDumpFile dir(/tmp)을 변경 -> disk가 충분한 path ```heapDump 용도로 /tmp 사용하는게 아니면 건들이지 말자...```

    5. 설치 시 경험기
        - hdfs의 permission 문제 발생 시
            - cm을 통해 hdfs의 만들어지는 파일 소유아래와 같다.
                - 소유계정 = hdfs
                - 소유 그룹 = supergroup
            - 정석적으로는 사용하는 계정을 hdfs으로 변경 혹은 소유 그룹에 계정을 넣어야한다.
            - 저의 경우에는 찾는법이 귀찮아서(사실 할줄몰라서..ㅎ) dfs.permissions의 설정값을 off 해놓았다.(hdfs의 권한 검사를 할지 option)
            
        - 스파크의 경우에는 각 executor들의 가용 heap사이즈의 따라 error(실제 경험기입니다.)
            
            - 1GB의 경우 spark 쉘 자체가 실행되지 않음 참고(https://www.cloudera.com/documentation/enterprise/latest/topics/spark_troubleshooting.html)
                <br>
                저의 경우에 VM을 통해 클러스터를 구축하였고 cloudera에서도 작은 VM으로 만들시 발생할 수 있으며 무료 cm에서는 아래와 같은 옵션을 주어 스파크 쉘에 진입하라고 되어있었습니다.<br>
                ```shell
                pyspark --executor-memory=500M
                ```
                이것은 각 executor의 heap 메모리를 지정하는 옵션입니다. <br>
                default는 1G 로 설정되어있습니다.

            - 500M의 경우 스파크 쉘은 진입되나 액션연산이 call되고 stage를 실행할때 error 발생.<br>
                1. 저의 경우는 600M로 올려 해결
                2. 문제로는 spark 작업을 돌릴때 메모리가 부족한 이유
                3. 부족했었던 이유는 개인적으로 아래와 같은 이유라고 생각
                    - 다른 작업도 하고 싶어 hdfs, spark, yarn을 제외하고 hive, hue, impala, oozie, zookeeper를 설치 및 서비스 on.
                    - 파이썬으로 rdd를 사용하여 trans 하는 과정에서의 부하.
    
        - cloudera manager 6.0v -> cloudera manager 6.2v update
            - 6.0의 경우 spark 2.2.0 제공, 쉘 실행하는 코드를 봐보면 실제로 deprecated된 코드 존재.
            - 6.1부터 spark 2.4.0 제공, 실제로 spark home에서 최신 tar.gz을 받아 까보면 코드 동일 확인.


