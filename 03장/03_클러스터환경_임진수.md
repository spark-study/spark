# 3장 클러스터 환경

#### 3.1 클러스터 환경

* 이전까지 사용하던 방식은 **로컬모드(standalone)**

* 여러대의 서버에서 동작하는 방식을 **클러스터 모드**
  * 클러스터 모드라고 해서 지금까지 했던 것이 달라지지 않음 → 분산 작업 관리 기능이 추가되는 것



##### 3.1.1 클러스터 모드와 컴포넌트

- **클러스터 매니저**

  - 전체 서버의 자원과 동작을 세밀하고 효율적으로 제어할 수 있는 모듈
  - 스파크에서 2.3.0 버전 기준으로 4개의 클러스터 매니저가 사용 중
  - 스파크에서는 클러스터 매니저를 추상화하여서 다른 매니저를 사용하여도 동일한 코드를 작성하고 실행하는 것이 가능

- **스파크의 프로그래밍 모델 관점**

  - 드라이버 프로그램 : **마스터 역할**

  - 익스큐터(스파크 프로세스) : **실제 데이터 처리**, 워커 노드에서 동작

    - *익스큐터는 스레드가 아닌 프로세스*
    - CPU, 메모리 등의 자원을 할당 단위 - **익스큐터 마다 자원을 정한 뒤 작업 실행 요청이 발생할 때 마다 익스큐터를 할당**

  - 스파크 컨텍스트 : 

    - 클러스터 매니저와의 연동, 작업 제어 and 자원을 관리하는 스케쥴러, 데이터 처리 및 전송 등 서비스를 준비

    - 스파크를 동작하기 위한 백그라운드 서비스 환경

    - *TIP : 여러개의 스파크컨텍스트를 생성하거나, 다른 어플리케이션과 스파크컨텍스트를 공유하는 것은 불가능*

      - but : 멀티 스레드에서 여러개의 job 실행가능, **심지어 Thread-safe 함*

      

* **클러스터 모드에서 작업 수행 개요**

  ​	[그림 링크](http://spark.apache.org/docs/latest/running-on-mesos.html)

  ![클러스터 모드](http://spark.apache.org/docs/latest/img/cluster-overview.png)

  1. 드라이버 프로그램이 포함된 어플리케이션 코드를 작성
  2. jar, zip등으로 패키징 → "어플리케이션 패키지 파일"
  3. spark-submit 셸을 이용하여 클러스터에 배포
  4. 드라이버 프로그램 실행 → 스파크컨텍스트 생성 → 클러스터 매니저 연동 → 익스큐터 프로세스 생성
     * 익스큐터를 생성한 어플리케이션에 할당
  5. 드라이버 프로그램에서 트랜스포메이션과 액션 연산 수행
     * job : 액션 연산이 호출될 때만 실제 작업을 수행하는데, 이때의 수행 단위
     * job은 액션의 수만큼 생성
  6. job을 스테이지 단위로 나누어서 실행, 이걸 태스크 단위로 다시 나우어 여러 익스큐터에 할당



##### 3.1.2 클러스터 모드를 위한 시스템 구성

* 스파크 클러스터 + 배치/ 클라이언트 서버로 구성
  * 이러한 서버들은 우지 or 젠킨스 같은 스케쥴링 서비스를 이용해 배포된 어플리ㅔ이션을 수행하고 기타 데이터 베이스나 시스템으로 전달하는 역할을 많이 수행



##### 3.1.3 드라이버 프로그램과 디플로이 모드

* 디플로이 모드 : 같은 어플리케이션도 클러스터 매니저에거 실행을 요청하는 방식을 다르게 할 수 있음
  1. 클라이언트 디플로이 모드
     * 드라이버 프로그램을 클라이언트에서 실행
       * 콘솔을 닫아 버리거나, 프로세스를 중지시키면 sc도 같이 종료되면서 스파크잡 중지
       * 스파크 쉘등은 해당 모드 사용
  2. 클러스터 디플로이 모드
     * 클라이언트에서 어플리케이션을 실행한 프로세스는 작업 요청후 종료되고, 클러스터 내부에서 드라이버 프로그램 구동
       * 드라이버 프로그램과 익스큐터 간의 메시지 전달하는 비용이 상대적으로 낮아, 성능 향상을 기대할 수 있음
       * 하지만, 정기적인 배치작업 등이 아니라면 일반적으로 클라이언트 모드 사용

------

#### 3.2 클러스터 매니저

* 클러스터 모드를 구성하는 콤포넌트 중 하나, 어플리케이션 간의 CPU나 메모리, 디스크 등의 컴퓨터 리소스를 관리하는 역할
* 하둡의 얀, 아파치 메소스, 스탠드얼론 클러스터 매니저, 쿠버네티스 매니저, 총 4가지 매니저 ( spark2.3.0 기준 )



##### 3.2.1 스탠드얼론 클러스터 매니저

* '마스터/슬레이브' 구성, 하나의 마스터 인스턴스와 다수의 슬레이브 인스턴스
  * 마스터 : 클라이언트 매니저의 역할
  * 슬레이브 : 워커 노드에 해당

* spark-shell, pyspark, spark-summit 에서는 —master <마스터 주소> 로 설정 가능
* 디플로이 모드
  * —deploy-mode 옵션으로 "cluster" / "client" 모드 중 하나를 지정 가능
* 스탠드얼론 클러스터 매니저는 주요 설정을 할 수 있는 방법을 제공
  * 실행 스크립트의 실행 매개변수 형태로 지정 or 미리 정의된 환경변수를 지정 가능
    * conf/spark-env.sh 파일에서 "export 변수=값" 형태로 지정가능
* HA(High Availability) 적용
  * 워커에서 문제가 발생하면 , 다른워커에게 전달하여 전체 작업에 문제 x
  * 마스터 자체에서 문제가 발생할 경우 작업은 복구되지 못하고 실패하게됨
    * HA 적용하지 않은 클러스터의 마스터는 SPOF(Single Point Of Failure)
  * 스탠드얼론 클러스터에서는 [주키퍼](https://bcho.tistory.com/1016)를 사용해 다수의 마스터 서버를 동일한 클러스터의 마스터로 지정하고 문제 발생시 다른 마스터로 전환할 수 이 있게 함
    * 주키퍼를 이용한 HA를 구현하려면 별도의 서버에 주키퍼 클러스터를 구성한 뒤 해당 주키퍼 서버의 정보를 spark-env.sh에 설정 해야함

* 단일 노드 복구
  * 주키퍼를 이용한 HA 구성은 하둡, HBase등에서 널리 활용되는 방법
  * but 안정적으로 운영을 위해서 3대 이상의 주키퍼 전용 서버를 투입해야함으로 부담이 존재
  * So, 장애 복구에 크게 민감하지 않은 서비스를 운영하는 경우, 주키퍼 대신 **단일 노드 복구 방법**을 사용해 장애 발생에 대응 가능
    * 로컬 디렉토리 특정위치에 클러스터의 상태 정보를 저장해 뒀다가 장애가 발생시, 저장된 정보를 이용해 다시 예전 상태로 복구하는 방식



##### 3.2.2 아파치 메소스

[메소스 논문](https://amplab.cs.berkeley.edu/wp-content/uploads/2011/06/Mesos-A-Platform-for-Fine-Grained-Resource-Sharing-in-the-Data-Center.pdf)

![image-20190724221922614](http://www.mimul.com/pebble/default/images/blog/tech/mesos_architecture.jpg)

* 메소스가 제공하는 서비스는 하나의 클러스터 자원을 여러 어플리케이션에서 공유해서 사용할 수 있게 해 주는 것
  * 특징은, 다양한 어플리케이션을 **"프레임워크"**라는 논리적인 컴포넌트로 인식하여, 하나의 서버에서 여러 어플리케이션을 실행하는 것처럼 환경을 제공
  * 하둡, 스파크 등을 프레임 워크로 인식을 하고 동일한 자원을 공유한 상태에서 필요한 만큼 자원을 나누어 사용할 수 있음
* 마스터, 슬레이브(에이전트) 구조
  * 마스터 : 메소스 마스터(주키퍼로 관리)
    * 여러 프레임워크와 슬레이브 사이에서 가용 자원을 확인하고 중계하는 방식으로 동작
    * 슬레이브가 가지고 있는 자원을 프레임워크에서 물어봐서 나누어 주고 남는것은 다른 프레임워크에게 할당
  * 슬레이브 : 메소스 프레임워크(태스크를 수행하는 역할)
    * 메소스 익스큐터가 스파크 익스큐터를 실행
    * 태스크를 처리 할 때, 프레임워크의 익스큐터를 여러개 만들거나 하나만 만들어서 슬레이브에서처리가능
      * 파인-그레인모드 : 익스큐터를 여러개
        * 태스크 마다 익스큐터를 생성하므로 매번 익스큐터를 초기화해야하지만 자원을 효율적으로 사용가능
      * 코어스-그레인모드 : 익스큐터를 하나를 만들고 하나로 여러 태스크를 수행
        * 빠르게 계속 작업을 시작가능, 하지만 자원을 계속 점유하게 됨



##### 3.2.3 얀

![얀 구조](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)



* 하둡2 이전에는 job Tracker, Task Tracker 프로세스를 이용해 맵리듀스 어플리케이션을 실행
  * job Tracker가 다수의 Task Tracker를 관리하여야 하고 , 전체 클러스터에서 하나의 job Tracker가 자원 할당, 실행 제어, 장애 복구, 작업 스케쥴링 등 을 수행해야 해서, 유연성과 확장성에 제약
  * yarn은 job tracker가 가지고 있던 이러한 기능을 각각 별개의 서비스로 분리한 것
  * 또한 API 기반 구조로 재작성하여 하둡과 맵리듀스 간 의존도를 줄이고 다른 어플리케이션도 구현할 수 있게 됨
* 크게 클라이언트 프로그램 + 리소스 매니저 + 노드 매니저 + 어플리케이션 마스터 + 컨테이너로 구성
  * 클라이언트 프로그램 : 리소스 매니저에게 어플리케이션 마스터를 실행해 달라고 요청하는 프로그램
  * 리소스 매니저 :  클러스터 전체의 리소스를 관리
  * 노드 매니저 : 워커 서버로, 요청받은 프로그램을 실행하는 컨테이너를 fork 시키고 해당 노드의 자원을 관리 보고
  * 어플리케이션 마스터 : 스파크의 드라이버 프로그램과 비슷한 역할, 해당 어플리케이션의 라이프 사이클을 관리하는 역할
  * 컨테이너 : 자원을 할당하는 단위, 컨테이너에서 각 익스큐터를 실행하게 됨



##### 3.2.5 쿠버네티스

* 스파크 2.3.0 부터 쿠버네티스 환경에서 스파크 어플리케이션 동작
* 오픈소스 컨테니어 오케스트레이션 솔루션
* 도커 
  * 리눅스 컨테이너를 기반으로한 일종의 가상화 기술
  * 도커 컨테이너는 도커 데몬 프로세스 상에서 각 컨테이너별로 하나의 고유한 작업을 담당하는 방식으로 동작하기 때문에 다수의 도커 데몬을 사용해야하는 클러스터 환경에는 적합하지 않음 → 쿠버네티스의 등장
* 쿠버네티스
  * 팟 : 여러 도커 컨테이너를 묶는 단위, 배포 단위
  * 노드 : 클러스터를 구성하는 서버
  * 서비스 : 여러개의 팟을 서비스하면서, 이를 로드 밸런서를 이용해서 하나의 IP와 포트로 묶어서 제공
  * 네임 서비스 : 쿠버네티스 내부에서 가상 클러스터를 식별하기 위한 용도로 사용
  * 스케줄러 : 각 노드의 리소스 사용률을 파악하고 적절한 노드에서 팟이 실행되게 관리
