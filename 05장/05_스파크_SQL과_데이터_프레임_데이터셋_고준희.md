---
layout: post
title:  "[스파크2 프로그래밍] 5장_스파크SQL 과 데이터프레임,데이터셋"
date:   2019-08-07
categories: Spark
---

RDD 의 장점은, 

- 분산환경에서 메모리 기반으로 빠르고 안정적으로 동작하는 프로그램을 만들 수 있다는 것과, 
- RDD 가 제공하는 풍부한 데이터 처리 연산입니다. ( <-> 맵과 리듀스로만 문제를 해결 ) 

RDD 의 단점은,

- 스키마를 표현할 수 없다는 것입니다. 

스파크 SQL 은 RDD 의 이 단점을 보완할 수 있도록 또 다른 유형의 데이터 모델과 API를 제공하는 스파크 모듈입니다. 스파크 SQL 에서 데이터를 다루는 방법은 SQL을 사용하는 것과 데이터셋 API 를 사용하는 방법이 있습니다. 

데이터셋은 스파크 1.6 버젼에서 처음 소개된 것으로 자바와 스칼라 언어에서만 사용할 수 있었고, 그 이전에는 데이터프레임이라는 클래스를 구현 언어와 상관없이 사용하고 있었습니다. **스파크 2.0 부터 데이터프레임 클래스가 데이터셋 클래스로 통합**되면서 Type Alias 라는 독특한 기능을 가진 스칼라 언어에서만 기존과 같은 데이터 프레임을 사용할 수 있고 해당 기능이 없는 자바에서는 데이터셋 클래스만을 사용할 수 있게 됐습니다. 

스칼라의 데이터프레임은 "type DataFrame = Dataset[Row]" 와 같이 정의돼 있는데 바로 이 구문이 Type Alias 에 해당하는 부분으로, **Dataset 의 타입 파라미터가 Row 인 경우를 DataFrame** 이라는 이름으로도 사용하겠다는 의미입니다.

```scala
val ds = List(1, 2, 3).toDS
ds.show
ds.printScheme
```

### 5.1 데이터셋

데이터셋 이전에는 데이터프레임이라는 API 를 사용했습니다. 가장 큰 특징은, 기존 RDD 와는 다른 형태를 가진 **SQL 과 유사한 방식의 연산을 제공**했다는 점입니다. 예를 들어,

```scala
rdd.map(v=>v+1)
```

와 같은 map() 연산을 사용한 것에 반해, 동일한 요소로 구성된 데이터프레임에서는 

```scala
df.select(df("value")+1)
```

와 같은 방법을 사용했습니다. df는 데이터프레임을 df("value") 는 데이터프레임이 가지고 있는 "value" 라는 이름의 칼럼을 의미하는 것으로, 마치 SQL 의 SELECT 문과 유사한 형식으로 데이터를 조회하는 방법입니다.

이러한 데이터 프레임의 장점은, 

- 풍부한 API 와 옵티마이저를 기반으로 한 높은 성능으로 복잡한 데이터 처리를 수월하게 수행할 수 있습니다.

하지만,

- 처리해야하는 작업의 특성에 따라 RDD 를 사용하는것에 비해 복잡한 코드를 작성하거나 컴파일 타임 오류 체크 기능을 사용할 수 없는 단점이 있습니다.

데이터셋은 데이터프레임이 제공하던 **성능 최적화 같은 장점을 유지하면서 RDD 에서만 가능했던 컴파일 타임 오류 체크 등의 기능을 사용**할 수 있게 됐습니다. 

### 5.2 연산의 종류와 주요 API

데이터셋이 제공하는 연산은 RDD 와 마찬가지로 두 종류로 분류합니다.

- 트렌스포메이션 
  - 새로운 데이터셋을 생성하는 연산. (액션 연산이 호출될 때까지 수행되지 않습니다)
- 액션연산
  - 실제 데이터 처리를 수행하고 결과를 생성하는 연산 

트랜스포메이션 연산은 데이터 타입을 처리하는 방법에 따라 두 가지로 나뉩니다.

- 타입 연산
- 비타입 연산

```scala
scala> val data = 1 to 100 toList
scala> val ds = data.toDS //데이터셋 생성
scala> val result = ds.map(_+1) // == ds.select(col("value") + 1)
```

```scala
ds.select(col("value") + 1)
```

이것은 데이터셋을 데이터베이스의 테이블과 유사하게 처리한 것으로, 마치 value 라는 칼럼을 가진 ds 라는 테이블에서 value 칼럼에 +1 을 한 결과를 조회하는 것과 유사한 방법입니다. 이 때, col("value") 라는 부분이 칼럼을 나타내며, 이 타입이 원래 데이터 타입인 정수가 아니라 org.apache.spark.sql.Column 타입입니다.

데이터셋에는 SQL 과 유사한 방식의 데이터 처리를 위해 데이터베이스의 Row 와 Column 에 해당하는 타입을 정의하고 있으며, 실제 데이터가 어떤 타입이든지 로우와 칼럼 타입으로 감싸러 처리할 수 있는 구조입니다.

**비타입 연산이란, 데이터를 처리할 때 데이터 본래의 타입이 아닌 org.apache.spark.sql.Column 과 org.apache.spark.sql.Row  타입의 객체로 감싸서 처리하는 연산**입니다.

데이터프레임은 org.apache.spark.sql.Row  타입의 요소로 구성된 데이터셋을 가리키는 용어입니다. 모든 데이터를 Row 타입으로 변환해서 생성된 데이터셋을 가리켜 데이터프레임이라는 별칭으로 부르기 대문에 이런 연산들을 데이터 프레임 연산이라고 부르기도 합니다.

### 5.3 코드 작성 절차 및 단어 수 세게 예제

단계별 스파크 SQL 코드 작성 방법입니다.

1. 스파크 세션 생성 (RDD 를 생성하기 위해 SparkContext가 필요한 것처럼, 데이터 프레임을 생성하기 위해 필요)

   ```java
   SparkSession spark = SparkSession
     .builder()
     .appName("jko") //application name
     .master("local[*]") //master info
     .getOrCreate();
   ```

2. 스파크세션으로부터 데이터셋 or 데이터프레임 생성

   ```java
   String source = "file://<spark_home_dir>/README.md";
   Dataset<Row> df = spark.read().test(source); //read() 는 DataFrameReader 인스턴스 리턴
   ```

3. 생성된 데이터셋 또는 데이터프레임을 이용해 데이터 처리

   ```scala
   val ds = df.as[(String)] // 데이터 프레임을 데이터 셋으로 변환. Row 타입 요소 -> 원래 데이터 타입
   val wordDF = ds.flatMap(_.split(" ")) // 문장을 각 단어로 분리
   val result = wordDF.groupByKey(v => v).count // 같은 단어끼리 그룹 생성하고 그룹별 요소 개수 count
   ```

4. 처리된 결과 데이터를 외부 저장소에 저장

   ```scala
   result.write.test("<저장경로>") // write() 는 DataFrameWriter 를 리턴
   ```

5. 스파크 세션 종료

#### 5.4 스파크세션

데이터프레임 또는 데이터셋을 생성하거나 사용자 정의 함수를 등록하기 위한 목적으로 사용됩니다. 스파크 SQL 2.0 부터 사용된 것으로, 기존에는 SQLContext 와 HiveContext를 사용했습니다.

기존 SQLContext 는 스파크세션과 유사하게 데이터 프레임을 생성하고 사용자 정의 함수를 등록하는 기능을 수행했는데, 아파치  하이브에서 제공하는 **HiveQL 을 사용하거나 기존 하이브 서버와 연동하기 위해서는 SQLContext 의 하위 클래스인 HiveContext 를 사용해야**했습니다.

이 두 클래스를 합친 **스파크세션 클래스를 정의하면서 스파크 세션 하나만으로 하이브 지원까지 가능**합니다. 

하이브 서버와 연동해서 사용하고 싶으면, 스파크의 conf 디렉토리에 hive-site.xml, core-site.xml., hfs-site.xml 파일을 생성해야 합니다. 하이브 및 하둡에서 사용하는 설정파일입니다.